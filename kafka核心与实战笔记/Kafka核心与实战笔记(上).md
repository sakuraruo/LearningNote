# Kafka核心与实战笔记

## 一、消息引擎系统ABC

1.消息引擎系统传输协议：点对点模型、发布订阅模型。

## 二、一篇文章带你快速搞定Kafka术语

1.Kafka属于分布式消息引擎系统，它的主要功能是提供一套完备的消息发布与订阅解决方案。

2.Kafka发布订阅的对象是主题(topic)。

3.向主题发布消息的客户端应用程序称为生产者(Producer),生产者程序通常持续不断地向一个或多个主题发送消息

4.订阅主题消息的客户端应用程序被称为消费者(Consumer)。

5.Kafka的服务器段由被称为Broker的服务进程构成，即一个Kafka集群由多个Broker组成。

6.Kafka的高可用基于Broker的分布式运行，其中一部分挂掉，并不影响服务可用性；另外就是备份机制，将数据拷贝到多台机器上。这些相同的数据被称为副本(Replica)。

7.Kafka定义了两种副本：领导者副本(Leader)和追随者副本(Follower)。

8.副本的工作机制：生产者总是向领导者副本写消息；而消费者总是从领导者副本读取消息。追随者副本只做数据冗余，请求领导者把最新生产的消息发给它，保持与领导者的同步。

9.Kafka的三层消息架构：第一层是主题层，每个主题可以配置M个分区，每个分区又可以配置N个副本；第二层是分区层，每个分区中的N个副本只能有一个充当领导者角色，对外提供服务，其他N-1个副本是追随者副本，只是提供数据冗余之用；第三层是消息层，分区包含若干条消息，每条消息的位移从0开始，依次递增。

10.Kafka持久化数据：kafka使用消息日志(Log)来保存数据，一个日志就是磁盘上一个只能追加写(Append-only)消息的物理文件。因为只能追加写入，故避免了随机IO操作，而是以顺序写入的操作，这也是Kafka高吞吐的一个重要手段。

11.kafka为了避免无限制的写入耗尽磁盘资源，kafka定期的删除消息以回收磁盘。通过日志段(Log Segment)机制。日志被切分城多个日志段，消息被追加到最新的日志段中，当写满一个日志段后，kafka会自动切分一个新的日志段，并将老的日志段封存起来。Kafka在后台还有定时任务会定期检查老的日志段能否被删除，从而实现回收磁盘空间的目的。

12.消费者位移：Consumer Offset。表征消费者消费进度，每个消费者都有自己的消费者位移。

13.消费组：Consumer Group。多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。

14.重平衡：Rebalance。消费组内某个消费者挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance是kafka消费端实现高可用的重要手段。



## 三、Kafka只是消息引擎系统吗？

1.Apache Kafka是消息引擎系统，也是一个分布式流处理平台。

2.Kafka设计之初就旨在提供三个方面的特性：提供一套API实现生产者和消费者；降低网络传输和磁盘存储开销；实现高伸缩架构。

3.从0.10.0.0版本正式推出了流处理组件Kafka Streams，正是从这时候开始kafka正是变身为分布式流处理平台，而不仅仅是消息引擎系统。



## 四、我应该选择哪种Kafka？

1.Apache Kafka,社区版Kafka。优势在于迭代速度快，社区响应度高，使用它让你有更高的把控度；切线在于仅提供基础核心组件，缺乏一些高级的特性。

2.Confluent Kafka，confluent公司提供的kafka。优势在于集成了很多高级特性且由kakfa原班人马打造，质量上有保证，缺陷在于相关文档资料不足，普及率低，没有太多可供参考的范例。

3.CDH、HDP Kafka,大数据云公司提供的Kafka，内嵌Apache kafka。优势在于操作简单，节省运维成本，缺陷在于把控度低，演进速度慢。



## 五、聊聊Kafka的版本号

1.Kafka共推出了7个大版本，分别是0.7、0.8、0.9、0.10、0.11、1.0和2.0

2.0.7版本-0.8版本引入了副本机制，kafka真正意义上成为了一个完备的分布式高可用消息队列解决方案。

3.0.8.2.0版本社区引入了新版本的Producer API，即需要指定Broker地址的Producer

4.国内依然有少部分用户在使用0.8.1.1、0.8.2版本，建议升级到0.8.2.2版本，该版本老版本Consumer API是比较稳定的，但是也不要使用新版本Producer API，此时bug挺多。

5.0.9.0.0版本，增加了基础的安全认证和权限功能，同时使用了Java重写了新版本Consumer API，引入了Kafka Connect组件用于实现高性能的数据抽取，这个版本可以使用新版本的Producer API，比较稳定了，但是仍然不要使用新版本Consumer API，bug还很多。

6.0.10.0.0是里程碑的大版本，该版本引入了kafka streams。0.10.2.2版本修复了一个可能导致Producer性能降低的bug，这个版本Consumer API基本稳定了。

7.0.11.0.0版本，引入了两个重量级的功能变更：一个是提供了幂等性Producer API和Transaction API；一个 是对kafka消息格式进行了重构。



## 六、Kafka线上集群部署方案怎么做？

1.Kakfa是由Scala和Java语言编写而成，不同操作系统的差异还是给Kafka集群带来了相当大的影响。

2.Kafka推荐部署到Linux上，原因处于：IO模型的使用、数据网络传输效率、社区支持度。

3.主流的IO模型通常由五种类型：阻塞式IO、非阻塞IO、IO多路复用、信号驱动IO和异步IO。

4.Linux中的select属于IO多路复用模型；epoll属于第三种和第四种之间，异步IO模型Linux很少有支持，在windows上提供了一个叫IOCP线程模型

5.Kafka客户端底层使用了Java中的selector，selector在linux上的实现机制式epoll，在windows上的实现是select，所以将kafka部署在linux上是有优势的。

6.Zero-Copy技术，实现了数据在磁盘和网络进行传输时避免昂贵的内核态数据拷贝从而实现快速地数据传输。Linux平台实现了这样的零拷贝机制，在windows平台要等到JDK8的60更新版本才能享受该机制。

6.Apache Kafka社区对于windows平台的bug修复不做任何承诺。

7.Kafka读写使用了顺序读写操作，故规避了机械硬盘的最大劣势，在考虑成本上，部署Kakfa不需要使用SSD。

8.磁盘阵列(RAID),优势在于：提供了冗余的磁盘存储空间、提供负载均衡。

9.Kafka自己通过副本机制实现了冗余机制来提供高可靠性，另外通过分区实现了软件层面的负载均衡，所以部署Kafka在考虑性价比时可以不需要组成磁盘阵列。

10.Kafka集群到底需要多大的存储空间，这是一个经典的规划问题。kafka需要将消息保存在底层的磁盘上，这些消息默认会被保存一段时间后自动删除，这个实现是可以配置的。如果每天一亿条消息，大小为1KB，保存两份且留存两周，那么总的空间大小就等于1亿* 1KB *14/1000/1000=200GB。一般来说kafka集群除了消息数据还有其他类型的数据，比如索引数据等，故我们再为这些数据预留出10%的磁盘空间，因此总的存储容量为220GB。既然要保存两周，那么整体容量即为220GB * 14，大约3TB左右。Kafka支持消息的压缩，假设压缩比为0.75，那么最后需要规划的存储空间大约为0.75 * 3=2.25TB。

11.规划磁盘容量时需要考虑下面这几个元素：新增消息数、消息留存时间、平均消息大小、备份数、是否启用压缩。

12.Kafka集群带宽资源的规划，其实是Kafka服务器数量规划。假设机房网络是千兆网络，1GBPS，业务目标是处理1TB数据每小时，需要多少台Kafka服务器来完成这个业务。假设每台机器Kafka占据70%的带宽，因为实际环境需要给其他进程留存资源。根据实际使用经验，带宽超过70%就有可能出现网络丢包的可能性了。由此计算单台Kafka最多使用700MB的带宽资源，这是它所能使用的最大带宽资源，你不能让Kafka服务器常规性使用这么多资源，故通常要再额外预留2/3的资源，即单台服务器使用带宽700MB/3≈240MBPS，其实2/3也过于保守了，可以稍微减小点预留资源。ITB每秒需要处理2336MB的数据，除以240，约等于10台服务器。因为集群，服务器台数还需要乘以3，30台。



## 七、最最最重要的集群参数配置（上）

Broker端参数：

1. log.dirs：指定了Broker需要使用的若干个文件目录路径。如果有条件的话最好保证这些目录挂载到不同的物理磁盘上。这样做有两个好处：提升读写性能，比起单块磁盘，多块磁盘同时读写拥有更高的吞吐量；能够实现故障转移：即failover，这是kafka1.1版本新引入的功能。在之前版本，只要kafka broker使用的任何一个磁盘挂掉了，整个Broker进程都会关闭。从1.1版本开始，坏掉的磁盘上的数据会自动地转移到其他正常的磁盘上，而且Broker还能正常功能。

2. log.dir：表示单个路径，补充上一个参数用的。

3. listeners：学名监听器，其实就是告诉外部连接者要通过什么协议访问指定主机名和端口开放的kafka服务

4. advertised.listeners:和listener相比多了个advertised。Advertised的含义表示宣称的、、公布的，就是说这组监听器是Broker用于对外发布的。

5. host.name/port：列出这两个参数就是想说你把它们忘掉吧，压根不要指定值，都是过期的参数。

   

   

   Topic参数：

   1.auto.create.topics.enable:是否允许自动创建Topic。

   最好设置为false，避免创建名字稀奇古怪的topic

   2.unclean.leader.election.enmable:是否允许Unclean Leader选举。

   建议显式设置为false，是否允许定期进行Leader选举。因为一旦保存数据比较多的副本都挂了，这时候再选举就会导致数据丢失

   3.auto.leader.rebalance.enable:是否允许定期进行Leader选举。

   建议生产设置为false，因为可能导致正常运行的副本Leader被更换。

   

   数据留存参数：

   1. log.retention.{hour|minutes|ms}：这是一个"三兄弟"，都是控制一条消息数据被保存多久时间
   2. log.retention.bytes:这是指定Broker为消息保存的总磁盘容量大小。
   3. message.max.bytes:控制Broker能够接受的最大单条消息大小。

   

   ## 八、最最最重要的集群参数配置（下）

   2.2版本kafka Topic参数

   1. 如果Topic参数与Broker参数都设置，哪个优先级高呢？topic参数会覆盖全局Broker参数。
   2. retention.ms:规定了该Topic消息被保存到时长，默认是七天。一旦设置了这个值，它会覆盖掉Broker端的全局参数值。
   3. retention.bytes：规定了要为该Topic预留多大的磁盘空间。默认值是-1；如果在多租户环境下就可以进行设置了。
   4. max.message.byte。它决定了kafka Broker能够正常接受该Topic的最大消息大小。

   

   JVM参数

   1.不推荐将Kafka运行在Java6、Java7上，kafka2.0.0开始已经正是摒弃了对Java7的支持了，所有尽量使用Java8。

   2.如果Broker所在的机器CPU资源非常充裕，尽量使用CMS收集器，启用方法是指定-XX:+UseCurrentMarkSweepGC。

   3.否则使用吞吐量收集器，开启的方法是指定-XX:+UseParallelGC。

   4.如果使用的是Jdk8，可以使用G1收集器。在没有任何调优的情况下，G1的表现比CMS出色，主要体现再更少的full GC，需要调整的参数更少，所以使用G1就好了。

   

   操作系统参数

   1.文件描述符限制 文件描述符系统资源并不像我们想的那么昂贵，设置一个超大的值是合理的做法，比如ulimit -n 1000000

   2.文件系统类型 根据官网的测试报告，XFS的性能要强于ext4，所以生产最好还是使用xfs

   3.Swappiness 关于交换内存的调优。网上很多文章都提到设置为0，将swap完全禁用掉以发放至kafka进程使用swap空间。但是推荐设置为一个接近0的值，比如1。因为一旦设置为0，当物理内存耗尽时，操作系统会出发OOM killer这个组件，它会随机挑选一个进程然后kill掉，即根本不给用户任何的预警。但如果设置城一个比较小的值，当开始使用swap空间时，你至少能够观测到Broker性能开始出现急剧下降，从而给出调优和诊断时间。

   4.提交时间 提交时间或者说flush落盘时间。向kafka发送数据并不是真要等数据被写入磁盘才算成功，而是只要数据被写入到操作系统的页缓存上（Page Cache）上就可以了，随后操作系统根据LRU算法会定期将页缓存上的"脏数据"落盘到物理磁盘。这个定期就是由提交时间来确定的，默认是5s。一般我们认为这个时间太频繁了，可以适当地增加提交间隔来降低物理磁盘的写操作。当然，如果数据还没写到磁盘机器宕机，数据就丢失了，但是kafka在软件层面已经提供了多副本的冗余机制，因此可以稍微拉大一点提交间隔去换取性能。



## 九、生产者消息分区机制原理剖析

1.为什么kafka要设计分区？为什么使用分区的概念而不是直接使用多个主题呢？其实分区的作用就是负载均衡的能力，或者说对数据进行分区的主要原因就是为了实现系统的高伸缩性(scalability)。不同的分区能够被放置到不同节点的机器上，而数据的读写操作也都是针对分区这个粒度而进行的，这样每个节点的机器都能独立地执行各自分区的读写请求处理。并且，我们还可以通过添加新的节点机器来增加整体系统的tps。

2.除了提供负载均衡这种最核心的功能外，利用分区也可以实现其他一些业务级别的需求，比如实现业务级别的消息顺序问题。

3.都有哪些分区策略？所谓分区策略是决定将生产者将消息发送到哪个分区的算法。kafka为我们提供了默认的分区策略，同时它也支持你自定义分区策略。如果要自定义分区策略，你需要显式地配置生产者端的参数partitioner.class。生产者程序时，可以实现org.apache.kafka.clients.producer.Partitioner接口。

4.轮训策略，也称Round-robin策略，即顺序分配，轮训策略是Kafka Java Produce API默认提供的分区策略。

5.随机策略，也称Randomness策略。

6.按照消息键保序策略。kafka允许为每条消息定义消息键(Key)。这个key的作用非常大，它可以是一个有着明确业务含义的字符串，消息指定相同的key之后，都会进入相同的分区。因为在同一个分区，都是严格保证其顺序性，故这个策略被称为按消息Key保序策略。



eg：根据地域不同，分发消息到不同的分区。

1.先根据Broker所在的IP地址实现定制化的分区策略，然后可以从所有分区中找出那些Leader副本符合规则的分区，随机挑选一个进行消息发送。



## 十、 生产者压缩算法面面观

1.压缩，其实就是秉承了用时间去换空间的经典trade-off思想，具体就是CPU时间去换磁盘空间或者网络IO的传输量。

2.kafka是如何压缩消息的？要弄清楚这个问题，就要从kafka的消息格式说起了。目前kafka共有两大类消息格式，社区称之为V1和V2。V2版本是kafka0.11.0.0中正式引入的。不管是哪个版本，kafka的消息层次都分为两层：消息集合(message set)以及消息(message)。一个消息几个钟包含若干条日志项(record item)，而日志项才是真正封装消息的地方。kafka底层的消息日志由一系列消息集合日志项组成。kafka通常不会直接操作具体的一条条消息，它总是在消息集合这个层面上进行写入操作。

3.何时压缩？在kafka中，压缩可发生在两个地方：生产端和Broker端。

生产端程序中配置compression.type参数即表示启用指定类型的压缩算法。producer启动后生产的每个消息几个都是经过GZIP压缩过的，故能很好地节省网络传输带宽和kafka Broker的磁盘占用。

4.可能让Broker端重新压缩消息

1）Broker端指定了和Producer端不同的算法。

2）Broker端发生了消息格式转换。

5.何时解压缩？Producer端压缩，Broker端保持，Consumer端解压缩。

6.各种压缩算法对比。在2.1.0版本之前，kafka支持三种压缩算法:GZIP、Snappy和LZ4，之后的版本正式支持Zstandard算法。

7.何时启用压缩？当CPU资源比较充足的情况下，可以开启。



## 十一、无消息丢失配置怎么实现？

1.一句话概括，kafka只对"已提交"的消息(committed message)做有限度的持久化保证。

2.第一个核心要素是"已提交的消息"。什么是已提交的消息？当kafka的若干个Broker成功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。此时这条消息在kafka看来就正式变为"已提交"消息了。

3.那为什么是若干个Broker呢？这取决于你对"已提交"的定义。你可以选择只要有一个Broker成功保存该消息就算是已提交，也可以使令所有Broker都成功保存该消息才算是已提交。不论哪种情况，kafka只对已提交的消息做持久化保证 这件事情是不变的。

4.第二个核心要素就是"有限度地持久化保证"，也就是说kafka不可能保证在任何情况下都做到不丢失消息。

5.kafka不丢消息是有前提条件的。假如你的消息保存在N个Kafka Broker上，那么这个前提条件是就是这N个Broker中至少有一个存活。只要这个条件成立，kafka就能保证你的这条消息永远不会丢失。

6.消息丢失案例

1）producer程序丢失数据：异步发送，发送失败，并不重试。

2）Consumer程序丢失数据：可能存在提交了位移，但是消息并未处理完就异常中断了。

3）Consumer程序开启多线程消费，其中一个线程失败了，导致消费未成功，却盲目的提交了位移。

7.如果是多线程异步处理消费消息，Consumer程序不要开启自动提交位移，而是要应用程序手动提交位移。

8.最佳实践

1）不要使用producer.send(msg),而要使用producer.send(msg,callback)。记住，一定要使用带有回调通知的send方法。

2）设置acks=all。acks是producer的一个参数，代表了你对"已提交"消息的定义。如果设置为all，则表明所有副本Broker都要接收到消息，该消息才算是"已提交"。这是最高等级的"已提交"定义。

3）设置retries为一个较大值。这里的retries同样是Producer的参数，对应前面提到的producer自动重试，当出现网络的瞬时抖动，消息发送可能会失败，此时配置了retries>0的Producer能够自动重试消息发送，避免消息丢失。

4）设置unclean.leader.election.enable=false。这是Broker端的参数，它控制的是哪些Broker有资格竞选分区的Leader。如果一个Broker落后原本的Leader太多，那么一旦它成为新的Leader，必然导致消息的丢失。

5）设置replication.factor>=3。这也是Broker端参数，这是设置分区副本数，目前防止消息丢失的主要机制就是冗余。

6)设置min.insync.replicas>1。这依然是broker端参数，控制的是消息至少要被写入到多少个副本才算是"已提交"。设置成大于1可以提升消息持久性，在实际环境千万不要使用默认值1。

7）确保replication.factor>min.insync.replicas。如果两者相等，那么只要有一个副本挂机，整个分区就无法正常工作了。我们不仅要改善消息的持久性，防止数据丢失，还要在不降低可用性的基础上完成。推荐设置成replication.factor=min.insync.replicas+1。

8）确保消息消费完成再提交。Consumer端有个enable.auto.commit，最好把它设置为false。



## 十二、客户端都有哪些不常见但是很高级的功能？

1.kafka拦截器分为生产者拦截器和消费者拦截器。生产者拦截器允许你在发送消息前以及消息提交成功后织入你的拦截器逻辑；而消费者拦截器支持在消费消息前以及提交位移后编写特定逻辑。值得一提的是，这两种拦截器都支持链的方式，即你可以将一组拦截器串连成一个大的拦截器，kafka会按照添加顺序依次执行拦截器逻辑。

2.org.apache.kafka.clients.producer.ProducerInterceptor接口，该接口是kafka提供的，里面有两个核心的方法。

1）onSend:该方法会在消息发送之前被调用。如果你想在发送之前对消息进行处理，这个方法是你唯一的机会。

2）onAcknowledgement:该方法会在消息成功提交或发送之后被调用。onAcknowledgement的调用要早于callback的调用。值得注意点是这个方法和onSend不是在同一个线程找那个被调用的，因此如果你在这两个方法中调用了某个共享可变对象，一定要保证线程安全。还有一点很重要，这个方法处在Producer发送的主路径中，所以最好别放一些太重的逻辑进去，否则producer的TPS会直线下降。

3.org.apache.kafka.clients.consumer.ConsumerInterceptor 接口，这里面也有两个核心方法。

1）onConsume: 该方法在消息返回给Consumer程序之前调用。也就是说在开始正式处理消息之前，拦截器会先拦一道，搞一些事情，之后再返回给你。

2）onCommit：Consumer在提交位移之后调用该方法。通常你可以在该方法中做一些记录的动作。

4.典型使用场景

1.kafka拦截器都能用在哪些地方呢？kafka拦截器可以应用与包括客户端监控、端到端系统性能检测、消息审计等多种功能在内的场景。



十三、消费者组到底是什么？

1.Consumer Group是kafka提供的可扩展且具有容错性的消费者机制。既然是一个组，那么组内必然可以有多个消费者或消费者实例(Consumer Instance),它们共享一个公共的ID，这个Id被称为Group ID。组内的所有消费者协调在一起来消费订阅主题(Subscribe Topic)的所有分区(Partition)。当然，每个分区只能由一个同一个消费组内的一个Consumer实例来消费。

1)Consumer Group下可以有一个或多个Consumer实例。这里的实例可以是一个单独的进程，也可以是同一个进程下的线程。在实际场景中，使用进程更为常见一些。

2）Group ID是一个字符串，在一个Kafka集群中，它标识唯一的一个Consumer Group。

3）Consumer Group下所有实例订阅的主题的单个分区，只能分配给组内的某个Consumer实例消费。这个分区当然也可以被其他Group消费。

2.传统消息引擎模型，分为点对点模型和发布、订阅模型。传统消息队列模型的缺陷在于消息一旦被消费，就会从队列中删除，而且只能被下游的一个Consumer消费。严格来说，这一点不算是缺陷，只能算是它的一个特性。但很显然，这种模式的伸缩性很差，因为下游的多个Consumer都要"抢"这个共享消息队列的消息。发布、订阅模型倒是允许消息被多个Consumer消费，但它的问题也是伸缩性不高，因为每个订阅者都必须要订阅主题的分区。这种全量的订阅方式既不灵活，也会影响消息的真实投递。

kafka的Consumer Group就是这样的机制。当Consumer Group订阅了多个主题后，组内的每个实例不要求一定要订阅主题的所有分区，它只会消费部分分区中的消息。Consumer Group之间彼此独立，互不影响，他们能够订阅相同的一组主题而不互相干扰，再加上broker的消息留存机制，kafka的Consumer Group完美地规避了上面提到的伸缩性差的问题。

3.理想情况下，Consumer 实例的数量应该等于该Group订阅主题的分区总数。

4.针对Consumer Group的设计特性，kafka是怎么管理位移的呢？消费者在消费的过程中需要记录自己消费了多少数据，即消费位置信息。在kafka中这个位置信息有个专门的术语：位移(Offset)。

5.对于Consumer Group而言，它是一组KV对，Key是分区，V对应Consumer消费该分区的最新位移。对应Java对象，即Map<TopicPartition,Long>，其中Topicpartition表示一个分区，而Long表示位移的类型

6.老版本的Consumer Group把位移保存在ZK中。Apache Zk是一个分布式的协调服务框架，kafka重度依赖它实现各种各样的协调管理。将位移保存在Zk外部系统的做法，最显而易见的好处就是减少了Kakfa Broker端的状态保存开销。现在比较流行的提法是将服务器节点做成无状态的，这样可以自由地扩缩容，超强的伸缩性。kafka最开始也是基于这样的考虑，才将Consumer Group位移保存在独立于Kafka集群之外的框架中。不过人们发现了一个问题，即ZK这类元框架并不适合进行频繁的写更新，而Consumer Group的位移更新却是一个非常频繁的操作。这种大吞吐量的写操作会极大地拖慢Zk集群的性能，因此Kakfa社区渐渐有了这样的共识：将Consumer位移保存在ZK中是不合适的做法。

7.在新版本的Consumer Group中，kafka社区重新设计了Consumer Group的位移管理方式，采用了将位移保存在kafka内部主题的方法。这个内部主题就是让人既爱又恨的_consumer_offsets。新版本的Consumer Group将位移保存在Broker端的内部主题中。

8.Rebalance本质上是一个协议，规定了一个Consumer Group下所有的Consumer如何达成一致，来分配订阅Topic的每个分区。

9.Rebalance触发的条件有3个。

1）组成员发生变更 2）订阅主题发生变更 3）订阅主题的分区数发生变更



## 十六、揭开神秘的“位移主题”面纱

1._consumer_offsets在kafka源码中有个更为正式的名字，叫做位移主题，即Offsets Topic。

2.老版本Consumer的位移管理是依托于Apache Zk的，它会自动或手动地将位移数据提交到ZK中。当Consumer重启后，它能自动从ZK中读取位移数据，从而在上次消费截止的地方继续消费。这种设计是的kafka Broker不需要保存任何位移数据，减少了broker端需要持有的状态空间，因而有利于实现高伸缩性。但是Zk并不适用于这种高频的写操作，因此，kafka社区自。0.8.2.x版本开始，就在酝酿修改这种设计，并最终在版本Consumer中正式推出了全新的位移管理机制，自然也包括这个新的位移主题。

3.新版本Consumer的位移管理机制其实也很简单，就是将Consumer的位移数据作为一条条普通的kafka消息，提交到_cosumer_offsets中。可以这么说，_consumer_offsets的主要作用是保存kafka消费者的位移消息。它要求这个提交过程不仅要实现高持久性，还要支持高频的写操作。

4.虽说位移主题是一个普通的kafka主题，但它的消息格式却是kafka自己定义的，用户不能修改，也就是说你不能随意地向这个主题写消息，因为一旦你写入的消息不满足kafka的格式，那么kafka内部无法成功解析，就会造成broker的崩溃。

5.首先从Key说起。一个kafka集群中的Consumer数量会有很多，既然这个主题保存的是Consumer的位移数据，那么消息格式中必须要有字段来标识这个位移数据是哪个customer的。这种数据放在哪个字段比较合适呢？显然放在key中比较合适。kafka中什么字段能够标识Consumer呢？没错就是，Consumer Group。

6.位移主题的Key中应该保存3部分的内容:<GroupId,主题名，分区名>。

7.当kafka集群中的第一个Consumer程序启动时，kafka会自动创建位移主题。我们说过，位移主题就是普通的kafka主题，那么它自然也有对应的分区数。但如果是kafka自动创建的，分区数是怎么设置的呢？这个要看Broker端参数Offsets.topic.num.partitions的取值。它默认值为50，因此kafka会自动创建一个50分区的位移主题。其副本因子取决于Broker端另外一个参数offsets.topic.replication.factor，默认是3。不过建议是，还是让kafka自动创建比较好。目前kafka源码中有一些地方硬编码了50分区，因此如果你自行创建了一个不同于默认分区数的位移主题，可能会碰到各种各样的问题。

8.Consumer提交位移的方式有两种：自动提交位移和手动提交。

9.Consumer端有个参数叫enable.auto.commit，如果值是true，则consumer在后台默默为你定期提交位移，提交间隔由一个专属的参数auto.commit.interval.ms来控制。自动提交有一个显著的优点，就是省事，但也因为省事，以至于丧失了很大的灵活性和可控性。

10.事实上，很多kafka集成的大数据框架都是禁用自动提交位移的，如spark、Flink等。这就引出了另一种位移提交方式:手动提交位移，即设置enable.auto.commit=false。kafka Consumer API为你提供了位移提交的方法,如Consumer.commitSync等。当调用这些方法时，kafka会向位移主题写入相应的方法。

11.如果你选择的是自动提交位移，那么可能存在一个问题：只要Consumer一直启动着，它就会无限期地向位移主题写入消息。假设Consumer当前消费到了某个主题的最新一条消息，位移是100，之后该主题没有任何新消息产生，故Consumer无消息可消费了，所以位移永远保持在100.由于是自动提交，位移主题中会不断写入位移=100的消息，显然kafka只需要保留这类消息中的最新一条就可以了，之前的消息都是可以删除的。这就要求kakfa必须要有针对位移主题消息特点的消息删除策略，否则这种消息会越来越多，最终撑爆整个磁盘。

kafka是怎么删除位移主题中的过期消息的呢？答案就是compaction。国内很多文献都将其翻译为压缩，其实更倾向于翻译为压实，或者干脆采用JVM垃圾回收中的术语：整理。

12.kafka使用compact策略来删除位移主题中的过期消息，避免该主题无限期膨胀。那么应该如何定义Compact策略中的过期呢？对于同一个Key的两条消息M1和M2，如果M1的发送时间早于M2，那么M1就是过期消息。Compact的过程就是扫描日志的所有消息，提出那些过期的消息，然后把剩下的消息整理在一起。

13.kafka提供了专门的后台线程定期的巡检待Compact的主题，看看是否存在满足条件的可删除数据。这个后台线程叫Log Cleaner。很多实际生产环境中都出现过位移主题无线膨胀占用过多磁盘空间的问题，如果你的环境中也有了这个问题，建议去检查下Log Cleaner线程的状态，通常都是这个线程挂掉了导致的。



## 十七、消费者组重平衡能避免吗

1.所谓协调者，在kafka中对应的术语是Coordinator，它专门为Consumer Group服务，负责为Group执行Rebalance以及提供位移管理和组成员管理。具体来说，Consumer端应用程序在提交位移时，其实是向Coordinator所在的broker提交位移。同样地，当Consumer应用启动时，也是向Coordinator所在的Broker发送各种请求，然后由Coordinator负责执行消费组的注册、成员管理记录等元数据管理操作。

2.所有Broker在启动时，都会创建和开启相应的Coordinator组件。也就是说，所有Broker都有各自的Coordinator组件。那么，Consumer Group如何确定为它服务的Coordinator在哪台broker上呢？答案就在之前说够的kafka内部位移主题_consumer_offsets身上。

3.目前，kafka为某个Consumer Group确定Coordinator所在的Broker的算法有两个步骤。

1）第一步，确定由位移主题的哪个分区来保存该Group数据：partitionId=Math.abs(groupId.hashCode()%offsetTopicPartitionCount)。

2.第二步，找出该分区Leader副本所在的Broker，该Broker即为对应的Coordinator。

解释一下上面的算法。首先，kafka会计算该Group的group.id参数的hash值。比如你有个Group的group.id设置成了"test-group",那么它的hashCode值就应该是627841412。其次，kafka会计算_consumer_offsets的分区数，通常是50个分区，之后将刚才那个hash值对分区数进行取模加求绝对值计算，即abs(627841412 % 50)=12。此时，我们就知道了位移主题的分区12负责保存这个Group的数据。有了分区号，算法第二步就变得很简单了，我们只需要找出位移主题分区12的Leader副本在哪个Broker上就可以了。这个Broker，就是我们要找的Coordinator。

4.在实际使用过程中，Consumer应用程序，特别是Java Consumer API，能够自动发现并连接正确的Coordinator，我们不用操心这个问题。知晓这个算法的最大意义在于，它能帮我们解决定位问题。当Consumer Group出现问题，需要快速排查Broker端日志时，我们能够根据这个算法准确定义Coordinator对应的Broker，不必一台Broker一台Broker地盲查。

5.如何避免Rebalance，那就说明Rebalance这个东西不好，那么，Rebalance的弊端是什么呢？

1）Rebalance影响Consumer端TPS。这个之前也反复提到了，这里就不再具体讲了。总之就是，在Rebalance期间，Consumer会停下手头的事情，什么也做不了。

2）Rebalance很慢。如果你的Group下成员很多，就一定会有这样的痛点。Group下有几百个Consumer实例，Rebalance一次要几个小时。在那种场景下，Consumer Group的Rebalance已经完全失控。

3）Rebalance效率不高。当前kafka的设计机制决定了每次Rebalance时，Group下的所有成员都要参与进来，而且通常不会考虑局部性原理。

6.如何避免Rebalance机制？

在真实的业务场景中，很多Rebalance都是计划外的后者说是不必要的。Rebalance发生的时机有三个：消费组组成员发生变化、订阅主题数量发生变化、订阅主题的分区发生变化。

如果Consumer Group下的Consumer实例数量发生变化，就一定会引发Rebalance。这是Rebalance发生的最常见的原因。遇到的99%的Rebalance，都是这个原因导致的。我们停掉Consumer，这是属于我们计划内的，不属于我们要规避的那种"不必要的Rebalance"。

在某些情况下，Consumer实例会被Coordinator错误地认为"已停止"从而被"踢出"Group。如果是这个原因导致的Rebalance，我们就不能不管了。

7.Coordinator会在什么情况下认为某个Consumer实例已挂从而要退组呢？

当Consumer Group完成Rebalance之后，每个Consumer实例都会定期向Coordinator发送心跳请求，表明它还存活着。如果某个Consumer实例不能及时地发送这些心跳请求，Coordinator就会认为该Consumer已经"死了"，从而将其从Group中移除，然后开启一轮新的Rebalance。Consumer端有个参数session.timeout.ms,就是被用来表征此事。该参数的默认值是10秒，即如果Coordinator在10秒之内没有收到Group下某个Consumer实例的心跳，它就会认为这个Consumer实例挂了。可以这么说session.timeout.ms决定了Consumer存活性的时间间隔。

除了这个参数，Consumer还提供了一个允许你控制发送心跳请求频率的参数，就是heartbeat.interval.ms。这个值设置的越小，代表Consumer实例发送心跳请求的频率越频繁。频繁地发送心跳请求会消耗额外的带宽资源，但好处是能够更快知晓当前是否开启Rebalance。Coordinator通知各个Consumer实例开启Rebalance的方法就是将REBALANCE_NEEDED标志封装进心跳请求的响应体中。

除了以上两个参数，Consumer端还有一个参数，用于控制Consumer实际消费能力对Rebalance的影响，即max.poll.interval.ms参数。它限定了Consumer端应用程序两次调用poll方法的最大时间间隔。

8.第一类非必要Rebalance是因为未能及时发送心跳，导致Consumer被"踢出"Group而引发的。因此你需要仔细地设置session.timeout.ms和heartbeat.interval.ms的值。这里给出无脑的推荐数值。

设置session.timeout.ms=6s

设置heartbeat.interval.ms=2s

要保证Consumer实例再被判定为"dead"之前，能够发送至少三轮心跳请求，即session.timeout.ms>=3*heartbeat.interval.ms。

9.第二类非必要Rebalance是Consumer消费时间过长导致的。

查看max.poll.interval.ms的参数设置，是否是因为消息处理时间过长，导致Consumer被踢出Group。

10.其他类别

如果按照上面的数值恰当的设置了这几个参数，却发现还是发生了Rebalance，那么建议去排查下Consumer端的GC表现，比如是否出现了频繁的Full GC导致长时间的停顿，从而导致Rebalance。



## 十九、CommitFailedException异常怎么处理？

1.所谓CommitFailedException，顾名思义就是Consumer客户端在提交位移时出现了错误或者异常，而且还是那种不可恢复的严重异常。

2.本次提交位移失败了，原因是消费者组已经开启了Rebalance过程，并且将要提交位移的分区分配给了另外一个消费者实例。出现这个情况的原因是，你的消费者实例连续两次调用poll方法的时间间隔超过了期望的max.poll.interval.ms参数值。这通常表明，你的消费者实例花费了太长的时间进行消息处理，耽误了调用poll方法。在后半部分，社区给出了两个相应的解决方法(即橙色部分)：

1)增加期望的时间间隔max.poll.interval.ms参数值。

2)减少poll方法一次性返回的消息数量，即减少max.poll.record参数值。

3.场景一

当消息处理的总时间超过预设的max.poll.interval.ms参数值时，kafka Consumer端会抛出CommitFailedException异常。这是该异常最"正宗"的登场方式。你只需要写一个Consumer程序，使用kafkaConsumer.subscribe方法随意订阅一个主题，之后设置Consumer端参数max.poll.interval.ms=5s,最后在循环调用kafkaConsumer.poll方法之间，插入Thread.sleep(6000)和手动提交，就可以成功复现这个异常了。

解决方案：

1）缩短单条消息处理的时间。比如，之前下游系统消费一条消息的时间为100ms，优化之后成功地下降到50ms，那么此时Consumer端的TPS就提升了一倍。

2.增加Consumer端允许下游系统消费一批消息的最大时长。这取决于Consumer端参数max.poll.interval.ms的值。在最新版的kafka中，该参数的默认值为5分钟。如果你的消费逻辑不能简化，那么提高该参数值是一个不错的方法。在kafka 0.10.1.0版本前是没有这个参数的，因此如果你需要在之前版本能使用，那么需要增加session.timeout.ms参数值。不幸的是，session.timeout.ms参数还有其他含义，因此增加该参数的值可能会有其他不良影响。这也是为啥从0.10.1.0版本后，将这部分语义从session.timeout.ms中剥离出来的原因。

3)减少下游系统一次性消费的消息总数。这取决于Consumer端参数max.poll.records的值。当前该参数的默认值是500条，表明调用一次kafkaConsumer.poll方法，最多返回500消息。可以说，该参数规定了单次poll方法能够返回的消息总数的上限。如果前两种方法对你来说都不适用的话，降低此参数值是避免CommitFailedException异常最简单的手段。

4）下游系统使用多线程来加速消费。这应该算是"最高级"同时也是最难实现的解决方法了。具体的思路就是，让下游系统手动地创建多个消费线程处理poll方法返回的一批消息。之前你使用kafka Consumer消费数据更多的是单线程的，所以当消费速度无法匹及kafka Consumer消息返回的速度时，它就会抛出CommitFailedException异常。如果是多线程，你就可以灵活地控制线程数量，随时调整消费承载能力，再配以目前多核的硬件条件，该方法可谓是放置commitFailedException最高档的解决之道。事实上，很多主流的大数据流处理框架使用的都是这个方法，比如Apache Flink在集成Kafka时，就是创建了多个kafkaConsumerThread线程，自行处理多线程间的数据消费。不过凡事有利就有弊，这个方法实现起来非常不容易，特别是在多个线程间如何处理位移提交这个问题上，更是极易出错。

4.场景二

还有一个冷门的不太为人所知的场景。独立消费者standalone Consumer也需要指定group id，消费组和独立消费组之前都需要指定group id。这时候如果你的应用中设置了相同的group.id值的消费者组程序和独立消费者程序，那么当独立消费者程序手动提交位移的时候，kafka就会立即抛出CommitFailedException。



## 二十、多线程开发消费者实例

1.谈到Java Consumer API，最重要的当属它的入口类KafkaConsumer了，我们说KafkaConsumer是单线程的设计，严格来说是不准确的。因为从kafka 0.10.1.0版本开始，kafkaConsumer就变为了双线程的设计，即用户主线程和心跳线程。

2.所谓用户主线程，就是你启动Consumer应用程序main方法的那个线程，而新引入的心跳线程(heartbeat Thread)只负责定期给对应的Broker机器发送心跳，以标识消费者应用的存活性(liveness)。引入这个心跳线程还有一个目的，那就是期望它能将心跳频率与主线程调用KakfaConsumer.poll方法的频率分开，从而解耦真实的消息处理逻辑与消费组成员存活性管理。

3.其实，在社区退出Java Consumer API之前，kafka中存在着一组统称为Scala Consumer的API。这组API，或者说这个Consumer，也被称为老版本Consumer，目前在新版的kafka代码中已经被完全移除了。之所以旧事重提，是想告诉你，老版本Consumer是多线程的架构，每个Consumer实例在内部为所有订阅的主题分区创建对应的消息获取线程，也称Fetcher线程。老版本Consumer同时也是阻塞式的(Blocking)，Consumer实例启动后，内部会创建很多阻塞式的消息获取迭代器。但在很多场景下，Consumer是有非阻塞需求的，比如在流处理逻辑中执行过滤(filter)、连接(join)、分组(group by)等操作时就不能是阻塞式的。基于这个原因，社区新版本Consumer设计了单线程+轮训的机制。这种设计能够较好地实现非阻塞式的消息获取。

4.多线程方案

了解了单线程的设计原理之后，我们来具体分析一下kafkaConsumer这个类的使用方法，以及如何推演出对应的多线程方案。首先，我们要明确的是，kafkaConsumer类不是线程安全(thread-safe)的。所有的网络IO处理都是发生在用户主线程中，因此你在使用过程中必须要确保线程安全。简单来说，就是你不能在多个线程中共享同一个kafkaConsumer实例，否则程序会抛出并发修改异常。当然这不是绝对的，kafkaConsumer中有个方法是例外的，那就是wake up(),你可以在其他线程中安全地调用kafkaConsumer.wakeup来唤醒Consumer。

鉴于kafkaConsumer不是线程安全的事实，我们能够制定两套多线程方案。

1）消费者程序启动多个线程，每个线程维护专属的kafkaConsumer实例，负责完整的消息获取、消息处理流程。

![image-20211025104947632](pic\image-20211025104947632.png)

2）消费者程序使用单或多线程获取消息，同时创建多个消费线程执行消息处理逻辑。获取消息的线程可以是一个，也可以是多个，每个线程维护专属的kafkaConsumer实例，处理消息则交由特定的线程池来做，从而实现消息获取与消息处理的真正解耦。

![image-20211025105425123](pic\image-20211025105425123.png)

总体来说，这两种方案都会创建多个线程，这些线程都会参与到消息的消费过程中，但各自的思路是不一样的。我们来打个比方。比如一个完整的消费者应用程序要做的事情是1、2、3、4、5，那么方案1的思路是粗粒度化的工作划分，也就是说方案一会创建多个线程，每个线程完整的执行1、2、3、4、5，以实现并行处理的目标，它不会进一步分割具体的子任务；而方案2则更细粒度化，它会将1、2分割出来，用单线程(也可以是用多线程)来做，对于3、4、5,则用另外的多个线程来做。



优劣势：

![image-20211025110050221](pic\image-20211025110050221.png)

方案一的优势有3点：

1.实现起来简单，因为它符合目前使用Consumer API的习惯。我们写代码的时候，使用多个线程并在每个线程中创建专属的kafkaConsumer实例就可以了。

2.多个线程之间使用专属的kafkaConsumer实例来执行消息获取和消息处理逻辑，因此，kafka主题中的每个分区都能保证只被一个线程处理，这就很容易区分分区内的消息消费顺序。这对在乎事件先后顺序的应用场景来说，是非常重要的优势。

3.多个线程之间彼此没有任何交互，省去了很多保证线程安全方面的开销。

方案一的不足：

1.每个线程都维护自己的kafkaConsumer实例，必然会占用更多的系统资源，比如内存、TCP连接等。在资源紧张的系统环境中，方案1的这个劣势会更加明显。

2.这个方案能使用的线程数受限于Consumer订阅主题的总分区数。我们知道，在一个消费者组中，每个订阅分区都只能被组内的一个消费者实例所消费。假设一个消费者组订阅了100个分区，那么方案1最多只能拓展到100线程，多余的线程无法分配到任何分区，只会白白消耗系统资源

3.每个线程完整地执行消息获取和消息处理逻辑。一旦消息处理逻辑很重，造成消息处理速度慢，就很容易出现不必要的Rebalance，从而引发整个消费者组的消费停滞。这个劣势你一定要注意。



方案2：

方案2将任务切分成了消息获取和消息处理两个部分，分别由不同的线程处理它们。比起方案1，方案2最大的优势在于它的高伸缩性，就是说我们可以独立地调节消息获取的线程数，以及消息处理的线程数，而不必考虑两者之间是否相互影响。如果你的消费获取速度慢，那么增加消费获取的线程即可；如果是消息的处理速度慢，那么增加worker线程池的线程数即可。

缺陷：

1.它的实现难度比方案1大的多，毕竟有两组线程，你需要分别管理它们。

2.因为该方案将消息获取和消息处理分开了，也就是说获取某条消息的线程不是处理该消息的线程，因此无法保证分区内的消费顺序。举个例子，比如在某个分区中，消息1在消息2之前被保存，那么Consumer获取消息的顺序必然是消息1在前，消息2在后，但是，后面的worker线程却有可能先处理消息2，再处理消息1，这就破坏了消息在分区中的顺序。还是那句话，如果你在意kafka中消息的先后顺序，方案2的这个劣势是致命了。

3.方案2引入了多组线程，使得整个消息消费链路被拉长，最终导致正确位移提交就会变得异常困难，结果就是可能会出现消息的重复消费。如果你在意这一点，那么我不推荐你使用方案2。



## 二十一、Java 消费者是如何管理TCP连接的?

1.何时创建TCP连接？

和生产者不同的是，构建kafkaConsumer实例时不会创建任何TCP连接。TCP连接是在调用KafkaConsumer.poll方法时被创建的。

过程:

1)发起FindCoordinator请求，希望kafka集群告诉它哪个Broker是管理它的协调者。理论上任意一台Broker都可以回答。kafka社区做了部分优化：消费者程序会向集群中当前负载最小的那台Broker发送请求。评估标准：谁的待发送请求最少。

2）连接协调者时。连接到Coordinator，接下来进入组协调操作。比如加入组、等待组分配方案、心跳请求处理、位移获取、位移提交等。

3）消费数据时。消费者会为每个要消费的分区创建于该分区领导者副本所在的Broker连接的TCP。



2.创建多少个TCP连接？

通常会创建三类TCP连接

1）确定协调者和获取集群元数据

2）连接协调者，令其执行组成员管理操作

3）执行实际的消息获取



3.何时关闭TCP连接？

和生产者类似，消费者关闭Socket也分为主动关闭和kakfa自动关闭。主动关闭是指你显式地调用消费者API的方法去关闭消费者，具体方式就是手动调用kafkaConsumer.close方法，或者实行kill 命令。kafka自动关闭是由消费者端参数connection.max.idle.ms控制，该参数默认值为9分钟，如果某个Socket连接上连续9分钟都没有任何请求，Consumer就会强行kill这个Socket连接。当第三类TCP连接成功创建之后，消费者程序就会废弃第一类TCP连接。

4.kafka有定时关闭机制。其实在实际场景中，见过很多将connection.max.idle.ms设置为-1，即禁用定时关闭的案例，如果是这样，这些TCP连接将不会被定期清除，只会成为永久的"僵尸"连接。



## 二十二、消费者组消费进度监控都怎么实现

1.所谓滞后程度，就是指消费者当前落后于生产者的程度。比如说，kafka生产者向某主题成功生产了100w条消息，你的消费者当前消费了80w条，那么我们就说你的消费者滞后了20w条消息，即Lag=20w。

2.通常来说，lag的单位是消息数。实际上kafka监控lag的层级是在分区上。如果要计算主题级别的，你需要手动汇总所有主题分区的lag并累加，合并为最终的lag值。

3.对于消费者而言，Lag应该算是最最最重要的监控指标了。它直接反映了一个消费者的运行情况。一个正常工作的消费者，它的Lag值应该很小，甚至是接近于0的，这标识该消费者能够及时地消费生产者生产出来的消息，滞后程度很小。反之，如果一个消费者Lag值很大，通常就表明它无法跟上生产者的速度，最终Lag会越来越大，从而拖慢下游消息的处理速度。更可怕的是，由于消费者的速度无法匹及生产者的速度，极有可能导致它消费的数据已经不在操作系统的页缓存中了，那么这些数据就会失去享有Zero Copy技术的资格。这样的话，消费者就不得不从磁盘上读取他们，这就进一步拉大了与生产者的差距，进而出现马太效应，即哪些lag原本就很大的消费者会越来越慢，lag也会越来越大。

4.你在实际业务场景中必须时刻关注消费者的消费进度。

5.kafka自带命令。kafka-consumer-group脚本是kafka为我们提供的最直接的监控消费者消费进度的工具。它也可以监控独立消费者(standalone Consumer)。

6.Kafka Java Consumer API

简单来说，社区提供了Java consumer API分别提供了查询当前分区最新消息位移和消费者组最新消费消息位移两组方法，我们使用它们就能计算出对应的Lag。

7.Kafka JMX监控指标

上面两种方式，都可以很方便地查询到给定消费者组的Lag信息。但是在很多实际监控场景中，我们借助的往往是线程的监控框架。如果是这种情况，以上两种办法就不怎么管用了，因为它们都不能集成进已有的监控框架中，如Zabbix或Grafana。下面我们就来看第三种办法，使用kafka默认提供的JMX监控指标来监控消费者的Lag值。

当然，kafka消费者提供了一个名为kafka.consumer:type=consumer-fetch-manager-metrics,client-id="{client-id}"的JMX指标，里面有很多属性。和Lag相关的两组属性：records-lag-max和records-lead-min，它们分别表示此消费者在测试窗口时间内曾经达到的最大的Lag值和最小的lead值。这里的lead值是指消费者最新消费消息的位移与分区当前第一条消息位移的差值。Lead越来越小也需要注意，这可能预示着消费者端要丢消息了。我们知道kafka的消息时有留存时间设置的，默认是一周，也就是说kafka默认删除一周前的数据。倘若你的消费者程序足够慢，慢到它要消费的数据快被kafka删除了。这时候一定会出现消息被删除，从而导致消费者程序重新调整位移值的情况。这可能会产生两个后果：一个是消费者从头消费一遍数据，另一个是消费者从最新的消息位移处开始消费，之前没来得及消费的消息全部被跳过了，从而造成丢消息的假象。

8.在生产环境中，请你一定要同时监控lag值和Lead值。



## 二十三、Kafka副本机制详解

1.所谓的副本机制(Replication),也可以称之为备份机制，通常是指分布式系统在多台网络互连机器上保存有相同的数据拷贝。副本机制有什么好处？

1）提供数据冗余。即使系统部分组件失效，系统依然能够继续运转，因而增加了整体可用性以及数据持久性。

2）提供高伸缩性。支持横向扩展，能够通过增加机器的方式来提升读性能，进而提供读操作吞吐量。

3）改善数据局部性。允许将数据放入与用户地理位置相近的地方，从而降低系统延时。

这些优点都是在分布式系统教科书中最常被提及的，但是有些遗憾的是，对于Apache Kafka而言，目前只能享受到副本机制带来的第一个好处，也就是提供数据冗余实现高可用性。

kafka Replication机制依然是Kafka设计架构的核心所在，它也是Kafka确保系统高可用和高持久性的重要基石。



2.副本定义

所谓副本，本质上就是一个只能追加写消息的提交日志。根据Kafka副本机制的定义，同一个分区下单所有副本保存有相同的消息序列，这些副本分散保存在不同的Broker上，从而能够对抗部分Broker宕机带来的数据不可用。

![image-20211025162139986](pic\image-20211025162139986.png)

3.副本角色

既然分区下能够配置多个副本，而且这些副本的内容还要一致，那么很自然的一个问题就是：我们该如何确保副本中的所有数据都是一致的呢？特别是对Kafka而言，当生产者发送消息到某个主题后，消息时如何同步到对应的所有副本中的呢?针对这个问题，最常见的解决方案就是基于领导者(Leader-based)的副本机制。Apache Kafka就是这样的设计。

![image-20211025162355949](pic\image-20211025162355949.png)

第一、在kafka中，副本分成两类：领导者(leader Replica)和追随者副本(Follower Replica)。每个分区在创建时都要选举一个副本，称为领导者副本，其余的副本自动称为追随者副本。

第二、kafka的副本机制比其他分布式系统要更严格一些。在kafka中，追随者副本是不对外提供服务的。这就是说，任何一个追随者副本都不能响应消费者和生产者的读写请求。所有的请求都必须由领导者副本来处理，或者说，所有的读写请求都必须发往领导者副本所在的broker，由该Broker负责处理。追随者副本不处理客户端请求，它位移的任务就是从领导者副本异步拉取消息，并写入到自己的提交日志中，从而实现与领导者副本的同步。

第三、当领导者副本挂掉了，或者说领导者副本所在的broker宕机时，kafka依托于Zk提供的监控功能能够实时感知到，并立即开启新一轮的领导者选举，从追随者副本中选一个作为新的领导者。老Leader副本重启回来后，只能作为追随者副本加入到集群中。

追随者副本是不对外提供服务的。



4.对于客户端而言，kafka的追随者副本没有任何作用，它既不能像MySQL那样帮助领导者副本"抗读"，也不能实现将某些副本放到离客户端近的地方来改善数据局部性。

既然如此，kafka为何要这样设计？其实这种副本机制有两个方面的好处。

1）方便实现"read your write"。

所谓Read your write，顾名思义，当你使用生产者API向Kafka成功写入消息后，马上使用消费者API去读取刚才生产的消息。

2）方便实现"Monotonic Reads"

什么是单调读呢？就是对于一个消费者用户而言，在多次消费消息时，它不会看到某条消息一会儿存在，一会儿不存在。



5.IN-sync Relicas  (ISR)

刚刚反复说过，追随者副本不提供服务，只是定期地异步拉取领导者副本中的数据而已。既然是异步的，就存在着不可能与Leader实时同步的风险。在探讨如何正确应对这种风险之前，我们必须要精确地知道同步的含义是什么。或者说，kafka要明确告诉我们，追随者副本到底要在什么条件下才算与leader副本同步。

基于这个想法，kafka引入了In-Sync Replicas，也就是所谓的ISR副本集合。ISR中的副本都是与Leader同步的副本，相反，不在ISR中的追随者副本就被认为是与Leader不同步的。那么到底什么副本能够进入到ISR中呢？

首先明确的是，Leader副本天然就在ISR中。也就是说，ISR不只是追随者副本集合，它必然包括Leader副本。甚至在某些情况下，ISR只有Leader这一个副本。



6.那么进入到ISR的追随者副本要满足一定的条件，是什么条件呢？

![image-20211025170110395](pic\image-20211025170110395.png)

图中有3个副本：一个Leader副本和两个追随者副本。Leader副本当前写入了10条消息，Follower1副本同步了其中6条消息，而Follower2副本只同步了其中的3条消息。现在，请你思考一下，对于这2个追随者副本，你觉得哪个追随者副本与Leader不同步?

答案是，要根据具体情况来定，换成英文，就是那句著名的"It depends"。看上去Follower2的消息数比Leader少很多，它是最有可能与Leader不同步的。的确是这样的，但仅仅是可能。

事实上，这张图中的2个Follower副本都有可能与Leader不同步，但也都有可能与Leader同步。也就是说，kafka判断Follower是否与leader同步的标准，不是看相差的消息数，而是另有玄机。

这个标准就是Broker端参数replica.lag.time.max.ms。这个参数的含义是Follower副本能够落后Leader副本的最长时间间隔，当前默认值是10s。也就说只要一个Follower副本落后Leader副本的时间不连续超过10s，那么kafka就认为该Follower副本与Leader是同步的，及时此时Follower副本中保存的消息明显少于leader副本中的消息。

Follower副本在被踢出ISR后，慢慢追上Leader的进度，它是能够重新被加回ISR的。这也表明，ISR是一个动态调整的集合，而非静态不变的。



6.Unclean 领导者选举(Unclean Leader Election)

既然ISR是可以动态调整的，那么自然就可以出现这样的情形：ISR为空。因为Leader副本天然就在ISR中了，如果ISR空了，就说明Leader副本也"挂掉了"，kafka需要重新选举一个新的Leader。可是ISR是空，此时该怎么选举新Leader呢？

kafka把所有不在ISR中的存活副本都称为非同步副本。通常来说，非同步副本落后Leader太多，从因此，如果选择这些副本作为新leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老Leader中的消息。在Kafka中，选举这种副本的过程被称为Unclean领导者选举。Broker端参数unclean.leader.election.enable控制是否允许unclean领导者选举。

通过开启unclean 领导者选举，可以提高服务端可用性，但是可能会造成数据丢失。

在实际情况下，推荐关闭。因为高可用性可以通过其他方法来实现。
